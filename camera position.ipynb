{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Member: Nguyễn Gia Minh, Phan Ngọc Lan Khanh, Phạm Đặng Yến Nhi "
      ],
      "metadata": {
        "id": "s_RbtJW4xHrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 1 Report**"
      ],
      "metadata": {
        "id": "AmiElFuOwkQR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkAleL6LwM55"
      },
      "source": [
        "## **Theory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCE7FnBHwM6A"
      },
      "source": [
        "The object as seen by\n",
        "the camera can be expressed by the homogenous transformation matrix T1\n",
        "\n",
        "$$\n",
        "T_1 = ^{\\mathrm{cam}}T_{\\mathrm{obj}}\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 1 & 0 & 1\\\\\n",
        "1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & -1 & 9\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The origin of the robot base coordinate system as seen by the camera can be represented\n",
        "by the homogenous transformation matrix T2\n",
        "\n",
        "$$T_2 = ^{\\mathrm{cam}}T_{\\mathrm{base}} =\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & -10\\\\\n",
        "0 & -1 & 0 & 20\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxheIpt2wM6B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#t1 is the homogenous transformation matrix of object to camera\n",
        "t1 = [[0, 1, 0, 1],\n",
        "      [1, 0, 0, 10],\n",
        "      [0, 0, -1, 9],\n",
        "      [0, 0, 0, 1]]\n",
        "\n",
        "#t2 is the homogenous transformation matrix of robot base to camera\n",
        "t2 = [[1, 0, 0, -10],\n",
        "      [0, -1, 0, 20],\n",
        "      [0, 0, -1, 10],\n",
        "      [0, 0, 0, 1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpmEjYIqwM6C"
      },
      "source": [
        "\n",
        "**a): After the equipment has been setup and these coordinate systems have been established, someone rotated the camera 90 degree about the z axis of the camera. What is the position and orientation of the camera with respect to the robot’s base coordinate system?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ISX6nFwM6D"
      },
      "source": [
        "We have T2 is the the homogenous transformation matrix of robot's base to camera. Therefore, in order to find the homogenous transformation matrix of the camera with respect to the robot's base coordinate system, we can invert the matrix T2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWTB8mNjwM6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ae6958-dd84-41d9-ab15-f57bdf3db1de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  0.,  0., 10.],\n",
              "       [-0., -1., -0., 20.],\n",
              "       [-0., -0., -1., 10.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#invert t2 to get the coordinate of the camera with respect to the robot base\n",
        "baseTcam = np.linalg.inv(t2)\n",
        "baseTcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us5BopNewM6E"
      },
      "source": [
        "Since the camera is rotated $90^°$ around the z-axis of the camera, the homogenous transformation matrix of the camera with respect to the robot's base coordinate system after the rotation is equal to $T_2\\_invert$ * $R_z$ (homogenous transformation matrix of the rotation matrix of $90^°$ around the z-axis of the camera).\n",
        "\n",
        "<br>\n",
        "\n",
        "$$^{\\mathrm{base}}T_{\\mathrm{cam}}=T_2\\_invert =\\left\\lbrack\n",
        "\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 10\\\\\n",
        "0 & -1 & 0 & 20\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$R_z =\\left\\lbrack\n",
        "\\begin{array}{cccc}\n",
        "\\cos (90) & -\\sin (90) & 0 & 0\\\\\n",
        "\\sin (90) & \\cos (90) & 0 & 0\\\\\n",
        "0 & 0 & 1 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack =\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & -1 & 0 & 0\\\\\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & 0 & 1 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ {^{\\mathrm{base}}T_{\\mathrm{cam}}}' = ^{\\mathrm{base}}T_{\\mathrm{cam}} * R_z =\n",
        "\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 10\\\\\n",
        "0 & -1 & 0 & 20\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack *\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & -1 & 0 & 0\\\\\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & 0 & 1 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack\n",
        "=\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & -1 & 0 & 10\\\\\n",
        "-1 & 0 & 0 & 20\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ5y2TN3wM6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cfef32-b8ba-4791-ac9e-068fe14e7f35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., -1.,  0., 10.],\n",
              "       [-1.,  0.,  0., 20.],\n",
              "       [ 0.,  0., -1., 10.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#homogenous transformation matrix of the rotation matrix of 90 degree around the z-axis of the camera\n",
        "Rz = [[0, -1, 0, 0],\n",
        "      [1, 0, 0, 0],\n",
        "      [0, 0, 1, 0],\n",
        "      [0, 0, 0, 1]]\n",
        "\n",
        "#the new homogenous transformation matrix of the camera with respect to the robot base = t2_invert * Rz\n",
        "baseTcam_ = np.matmul(baseTcam,Rz)\n",
        "baseTcam_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi8YkAu4wM6G"
      },
      "source": [
        "**b) After the rotation happened in a), the object is also rotated by 90 degree about the x axis of the object and translated by 4 units along the rotated y axis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brcU48VBwM6H"
      },
      "source": [
        "**What is the position and orientation of the object with respect to the robot’s base coordinate system?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNFJBUhswM6H"
      },
      "source": [
        "We have $Z_x$ is the homogenous transformation matrix of the rotation matrix of $90^°$ around the x-axis of the object and $T_y4$ is the homogenous transformation matrix of the translation matrix of 4 units along the rotated y-axis.\n",
        "\n",
        "<br>\n",
        "\n",
        "$$R_x =\\left\\lbrack\n",
        "\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & \\cos (90) & -\\sin (90) & 0\\\\\n",
        "0 & \\sin (90) & \\cos (90) & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack =\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & 0 & -1 & 0\\\\\n",
        "0 & 1 & 0 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$T_{y4} =\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & 1 & 0 & 4\\\\\n",
        "0 & 0 & 1 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \\\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soz4cNCGwM6I"
      },
      "outputs": [],
      "source": [
        "#rotation matrix of 90 degree around x-axis of the object\n",
        "Rx = [[1, 0, 0, 0],\n",
        "      [0, 0, -1, 0],\n",
        "      [0, 1, 0, 0],\n",
        "      [0, 0, 0, 1]]\n",
        "\n",
        "#translation matrix by 4 unit along the y axis\n",
        "T_y4  = [[1, 0, 0, 0],\n",
        "      [0, 1, 0, 4],\n",
        "      [0, 0, 1, 0],\n",
        "      [0, 0, 0, 1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL2DxZxswM6I"
      },
      "source": [
        "Firstly, we find the homogenous transformation matrix of the object with respect to the robot's base coordinate using the following equation: $^{\\mathrm{base}}T_{\\mathrm{obj}} = ^{\\mathrm{base}}T_{\\mathrm{cam}} * ^{\\mathrm{cam}}T_{\\mathrm{obj}}  = T_2^{-1} * T_1 $\n",
        "\n",
        "<br>\n",
        "\n",
        "$$^{\\mathrm{base}}T_{\\mathrm{obj}} =\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 10\\\\\n",
        "0 & -1 & 0 & 20\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \n",
        "\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 1 & 0 & 1\\\\\n",
        "1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & -1 & 9\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \n",
        "=\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 1 & 0 & 11\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & 1 & 1\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKKxa4aRwM6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7a01f6-c74f-4d0b-f737-8b302914cb8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  1.,  0., 11.],\n",
              "       [-1.,  0.,  0., 10.],\n",
              "       [ 0.,  0.,  1.,  1.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#the homogenous transformation matrix of the object with respect to the robot's base coordinate\n",
        "baseTobj = np.matmul(baseTcam,t1)\n",
        "baseTobj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP-mYe-hwM6J"
      },
      "source": [
        "Then we will multiply the matrix $^{\\mathrm{base}}T_{\\mathrm{obj}} $ with $R_x$ to get the new matrix ${^{\\mathrm{base}}T_{\\mathrm{obj}}}' $. After that, we can find the new position and orientation of the object with respect to the robot's base coordinate by multiply ${^{\\mathrm{base}}T_{\\mathrm{obj}}}'$ with $T_{y4}$ to get the ${^{\\mathrm{base}}T_{\\mathrm{obj}}}''$\n",
        "\n",
        "<br>\n",
        "\n",
        "$${^{\\mathrm{base}}T_{\\mathrm{obj}}}'\n",
        "=\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 1 & 0 & 11\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & 1 & 1\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \n",
        "\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & 0 & -1 & 0\\\\\n",
        "0 & 1 & 0 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack\n",
        "= \\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 0 & -1 & 11\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 1 & 0 & 1\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L68xqO-2wM6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809a232f-b0c4-4e87-e494-70cfbcd5f350"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0., -1., 11.],\n",
              "       [-1.,  0.,  0., 10.],\n",
              "       [ 0.,  1.,  0.,  1.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "baseTobj_ = np.matmul(baseTobj,Rx)\n",
        "baseTobj_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRaEvk50wM6K"
      },
      "source": [
        "$$ {^{\\mathrm{base}}T_{\\mathrm{obj}}}''\n",
        "=\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 0 & -1 & 11\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 1 & 0 & 1\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \n",
        "\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0\\\\\n",
        "0 & 1 & 0 & 4\\\\\n",
        "0 & 0 & 1 & 0\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack\n",
        "= \\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 0 & -1 & 11\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 1 & 0 & 5\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjx7-vSAwM6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f044621-6834-426f-b60d-07d27771eb3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0., -1., 11.],\n",
              "       [-1.,  0.,  0., 10.],\n",
              "       [ 0.,  1.,  0.,  5.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#the position and orientation of the object with respect to the robot base after rotated\n",
        "baseTobj_final = np.matmul(baseTobj_, T_y4)\n",
        "baseTobj_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIH6qbT-wM6K"
      },
      "source": [
        "**What is the position and orientation of the object with respect to the rotated camera coordinate system?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLBUJ2FwM6L"
      },
      "source": [
        "First we will invert the ${^{\\mathrm{base}}T_{\\mathrm{cam}}}'$ matrix, which is the rotated camera with respect to the object coordinate system to get the homogenous transformation matrix of the ${^{\\mathrm{cam}}T_{\\mathrm{base}}}'$ - the position and orientation of the object with respect to the rotated camera coordinate system.\n",
        "\n",
        "<br>\n",
        "\n",
        "$${^{\\mathrm{cam}}T_{\\mathrm{base}}}' =\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & -1 & 0 & 20\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UFPeeBBwM6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb9006f-6e16-493d-d1da-8688cb9b5f27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0., -1., -0., 20.],\n",
              "       [-1., -0., -0., 10.],\n",
              "       [-0., -0., -1., 10.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#invert Rot_cam_to_base\n",
        "camTbase_ = np.linalg.inv(baseTcam_)\n",
        "camTbase_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr-hY2pzwM6L"
      },
      "source": [
        "Then we can get the position and orientation of the object with respect to the rotated camera coordinate system by multiply ${^{\\mathrm{cam}}T_{\\mathrm{base}}}'$ and ${^{\\mathrm{base}}T_{\\mathrm{obj}}}''$\n",
        "\n",
        "<br>\n",
        "\n",
        "$${^{\\mathrm{cam}}T_{\\mathrm{obj}}}' = {^{\\mathrm{cam}}T_{\\mathrm{base}}}'* {^{\\mathrm{base}}T_{\\mathrm{obj}}}'' =\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & -1 & 0 & 20\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & -1 & 10\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \n",
        "\\left\\lbrack \\begin{array}{cccc}\n",
        "0 & 0 & -1 & 11\\\\\n",
        "-1 & 0 & 0 & 10\\\\\n",
        "0 & 1 & 0 & 5\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack \n",
        "=\\left\\lbrack \\begin{array}{cccc}\n",
        "1 & 0 & 0 & 10\\\\\n",
        "0 & 0 & 1 & -1\\\\\n",
        "0 & -1 & 0 & 5\\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right\\rbrack$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd0Ea1ulwM6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348b6fb5-d4b2-4c36-cf4d-cd2444cb17a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  0.,  0., 10.],\n",
              "       [ 0.,  0.,  1., -1.],\n",
              "       [ 0., -1.,  0.,  5.],\n",
              "       [ 0.,  0.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#the position and orientation of the rotated object with respect to the rotated camera\n",
        "camTobj_ = np.matmul(camTbase_, baseTobj_final)\n",
        "camTobj_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvji2zftwM6M"
      },
      "source": [
        "## **Programming**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8wJAONcwM6M"
      },
      "source": [
        "### Task 1: Camera Calibration & Undistort image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfJ-0SVwM6M"
      },
      "source": [
        "When an image is captured by a camera, it is often distorted due to the optical characteristics of the camera lens. This distortion can cause straight lines to appear curved, objects to appear stretched or compressed, and can affect the accuracy of measurements taken from the image. To correct for this distortion, we can undistort the image using a calibration process that involves finding the camera matrix and distortion coefficients of the camera. Once we have these parameters, we can use them to undistort images captured by the same camera and improve their visual appearance and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvfXyahwM6N"
      },
      "source": [
        "The code below is for camera calibration and distortion correction using OpenCV. The process involves capturing a set of chessboard images from the camera, detecting the corners of the chessboard in the images, and then using the detected corners to calibrate the camera and obtain the camera matrix and distortion coefficients. The camera matrix and distortion coefficients can be used to undistort any subsequent images taken with the same camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ4ogRGFwM6N"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import glob\n",
        "import yaml\n",
        "\n",
        "def calibrate(folder, CHESSBOARD_CORNER_NUM_X, CHESSBOARD_CORNER_NUM_Y, criteria ):\n",
        "    CAMERA_PARAMETERS_OUTPUT_FILE = \"cam.yaml\"\n",
        "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(8,5,0)\n",
        "    objp = np.zeros((CHESSBOARD_CORNER_NUM_X*CHESSBOARD_CORNER_NUM_Y,3), np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:CHESSBOARD_CORNER_NUM_X,0:CHESSBOARD_CORNER_NUM_Y].T.reshape(-1,2)\n",
        "\n",
        "    # Arrays to store object points and image points from all the images.\n",
        "    objpoints = [] # 3d point in real world space\n",
        "    imgpoints = [] # 2d points in image plane.\n",
        "    images = glob.glob(folder)\n",
        "    for fname in images:\n",
        "        img = cv.imread(fname)\n",
        "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Find the chess board corners\n",
        "        ret, corners = cv.findChessboardCorners(gray, (CHESSBOARD_CORNER_NUM_X,CHESSBOARD_CORNER_NUM_Y), None)\n",
        "\n",
        "        # If found, add object points, image points (after refining them)\n",
        "        if ret == True:\n",
        "            objpoints.append(objp)\n",
        "            corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n",
        "            imgpoints.append(corners2)\n",
        "            # Draw and display the corners\n",
        "            cv.drawChessboardCorners(img, (CHESSBOARD_CORNER_NUM_X,CHESSBOARD_CORNER_NUM_Y), corners2, ret)\n",
        "        else:\n",
        "            print('Failed to find a chessboard in {}'.format(fname))\n",
        "    \n",
        "    # Draw lines to check distortion\n",
        "    draw_line(img, corners)\n",
        "    \n",
        "    #get the camera matrix\n",
        "    ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
        "    print(\"Camera matrix is \\n\", mtx, \"\\n And is stored in calibration.yaml file along with distortion coefficients : \\n\", dist)\n",
        "    mean_error = 0\n",
        "    for i in range(len(objpoints)):\n",
        "        imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\n",
        "        error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2)\n",
        "        mean_error += error\n",
        "    print( \"total error: {}\".format(mean_error/len(objpoints)) )\n",
        "    data = {'camera_matrix': np.asarray(mtx).tolist(), 'dist_coeff': np.asarray(dist).tolist()}\n",
        "    with open(CAMERA_PARAMETERS_OUTPUT_FILE, \"w\") as f:\n",
        "        yaml.dump(data, f)\n",
        "        \n",
        "    return mtx, dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrY_Hyf8wM6O"
      },
      "source": [
        "The ***calibrate()*** function is an important function used in the camera calibration process that takes in the folder containing the chessboard images, the number of corners in the chessboard in X and Y directions, and the calibration criteria. Once the images are read into the function, it converts them to grayscale, and detects the corners of the chessboard in the image using the ***cv.findChessboardCorners()*** function. The detected corners may not be accurate enough for calibration due to noise and other factors. Therefore, the ***cv.cornerSubPix()*** function is used to refine the corners to subpixel accuracy using an iterative algorithm.\n",
        "\n",
        "After detecting the corners using ***cv.findChessboardCorners()*** and refining the corners using ***cv.cornerSubPix()***, the ***cv.drawChessboardCorners()*** function is used to draw the detected corners on the image. This function takes in the original image, the number of corners in the chessboard in X and Y directions, the detected corners, and a flag to specify whether to draw the corners as a connected line or as individual circles. The function then draws the corners on the image and returns the image with the drawn corners.\n",
        "\n",
        "Once the corners are successfully detected and refined, the object points (known 3D coordinates of the chessboard corners) and image points (detected 2D coordinates of the corners in the image) are added to the ***objpoints*** and ***imgpoints*** lists, respectively. These lists are then used to calibrate the camera using the ***cv.calibrateCamera()*** function, which returns the camera matrix and distortion coefficients. This function takes in the object points and image points obtained from the detected corners of the chessboard images and calculates the camera matrix and distortion coefficients. The camera matrix contains intrinsic parameters such as the focal length of the camera, the optical center, and the skew of the image plane. The distortion coefficients represent any distortion in the camera lens, such as radial distortion and tangential distortion.\n",
        "\n",
        "The ***cv.calibrateCamera()*** function minimizes the error between the object points and the corresponding image points by adjusting the camera matrix and distortion coefficients. This process is called camera calibration and is typically done using a set of calibration images with known 3D object points. Once the camera matrix and distortion coefficients are obtained, they can be used to undistort any subsequent images taken with the same camera. Undistorting an image involves applying the inverse of the distortion coefficients to correct for any distortion in the camera lens.\n",
        "\n",
        "The camera matrix and distortion coefficients are then saved in a YAML file named \"cam.yaml\" using the ***yaml.dump()*** function. Overall, the ***calibrate()*** function plays an essential role in the camera calibration process, and it is important to ensure accurate detection and refinement of the chessboard corners to obtain accurate camera parameters for subsequent image processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfFgMYvBwM6O"
      },
      "outputs": [],
      "source": [
        "#undistort an image\n",
        "def undistort(img, mtx, dist):\n",
        "    h,  w = img.shape[:2]\n",
        "    newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n",
        "    dst = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
        "    x, y, w, h = roi\n",
        "    dst = dst[y:y+h, x:x+w]\n",
        "    cv.imwrite('undistort.png', dst)\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(img)\n",
        "    plt.title('distorted')\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(dst)\n",
        "    plt.title('undistorted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lS4baaIwM6O"
      },
      "source": [
        "The function ***undistort()*** takes in an image, camera matrix (mtx), and distortion coefficients (dist) as parameters. The function first gets the height and width of the input image using the shape attribute.\n",
        "\n",
        "Next, the function uses the ***cv.getOptimalNewCameraMatrix()*** function to obtain a new camera matrix and region of interest (ROI) for the undistorted image. The function takes in the original camera matrix, distortion coefficients, size of the new image (in this case, the same as the input image), scale factor of the new camera matrix (set to 1 to retain the same scale as the original camera matrix), and optional argument for specifying the ROI. The new camera matrix is optimized to minimize the distortion in the undistorted image.\n",
        "\n",
        "Then, the function uses the ***cv.undistort()*** function to undistort the input image using the original camera matrix, distortion coefficients, and the new camera matrix obtained from the previous step. The function returns the undistorted image in the variable dst.\n",
        "\n",
        "After undistorting the image, the function extracts the region of interest from the undistorted image using the ROI obtained from the ***cv.getOptimalNewCameraMatrix()*** function. This is done using slicing operations on the dst variable.\n",
        "\n",
        "Finally, the function saves the undistorted image as \"undistort.png\" using the ***cv.imwrite()*** function, and displays the original and undistorted images side by side using matplotlib. The original image is displayed using the ***plt.subplot()*** and ***plt.imshow()*** functions with the title \"distorted\", while the undistorted image is displayed with the title \"undistorted\".\n",
        "\n",
        "Overall, this function is used to undistort an image based on the camera matrix and distortion coefficients obtained from the camera calibration process using the ***calibrate()*** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhJmV0qiwM6P"
      },
      "outputs": [],
      "source": [
        "def draw_line(img, corners):\n",
        "    # Draw lines to check distortion\n",
        "    cv.line(img, tuple(map(int, corners[0, 0])), tuple(map(int, corners[45, 0])), (0, 0, 255), 1)\n",
        "    cv.line(img, tuple(map(int, corners[8, 0])), tuple(map(int, corners[53, 0])), (0, 0, 255), 1)\n",
        "    cv.line(img, tuple(map(int, corners[0, 0])), tuple(map(int, corners[8, 0])), (0, 0, 255), 1)\n",
        "    cv.line(img, tuple(map(int, corners[45, 0])), tuple(map(int, corners[53, 0])), (0, 0, 255), 1)\n",
        "    plt.imshow(img);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEh-Zp07wM6P"
      },
      "source": [
        "The ***draw_line()*** function takes in an image and the detected corners of a chessboard in that image. The function then draws lines on the image to check for distortion. The lines are drawn between specific corners of the chessboard, which are identified using their index in the corners array.\n",
        "\n",
        "The ***cv.line()*** function is used to draw the lines on the image. This function takes in the image, the starting and ending points of the line, the color of the line, and the thickness of the line. The starting and ending points of the lines are specified by selecting specific corners from the corners array using their index. The color of the lines is specified as (0, 0, 255), which represents red, and the thickness of the lines is set to 1.\n",
        "\n",
        "Finally, the function displays the original image with the lines drawn on it using ***plt.imshow()***. The lines drawn on the image can be used to visually check for any distortion that may be present in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzIoGiAKwM6P"
      },
      "outputs": [],
      "source": [
        "CHESSBOARD_CORNER_NUM_X = 9\n",
        "CHESSBOARD_CORNER_NUM_Y = 6\n",
        "\n",
        "# termination criteria\n",
        "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e99VoL-2wM6P"
      },
      "source": [
        "The code sets two constants, ***CHESSBOARD_CORNER_NUM_X*** and ***CHESSBOARD_CORNER_NUM_Y***, to specify the number of corners in the chessboard in the X and Y directions, respectively. These constants are then used in the ***calibrate()*** function to detect the corners of the chessboard in the calibration images.\n",
        "\n",
        "The code also sets up a termination criteria for the corner refinement process. This is done using the ***cv.TERM_CRITERIA_EPS*** and ***cv.TERM_CRITERIA_MAX_ITER*** constants, which specify the maximum number of iterations and the required accuracy for the corner refinement process. These criteria are used in the ***cv.cornerSubPix()*** function to iteratively refine the corners to subpixel accuracy.\n",
        "\n",
        "The termination criteria are important to ensure that the corner refinement process stops once the desired accuracy is achieved. Without these criteria, the corner refinement process may continue indefinitely, which can lead to overfitting and incorrect calibration results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5a58Cv8wM6Q",
        "outputId": "cbce5cce-24c3-4cd5-d5dc-1c7c67ac11a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f6a89fafd637>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./cam2_images/*.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHESSBOARD_CORNER_NUM_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHESSBOARD_CORNER_NUM_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-2f6109978acd>\u001b[0m in \u001b[0;36mcalibrate\u001b[0;34m(folder, CHESSBOARD_CORNER_NUM_X, CHESSBOARD_CORNER_NUM_Y, criteria)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Draw lines to check distortion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdraw_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#get the camera matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'img' referenced before assignment"
          ]
        }
      ],
      "source": [
        "folder = './cam2_images/*.jpg'\n",
        "mtx, dist = calibrate(folder, CHESSBOARD_CORNER_NUM_X, CHESSBOARD_CORNER_NUM_Y, criteria )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StjghdtjwM6R"
      },
      "source": [
        "The generated image shows the detected corners and the lines drawn on the chessboard to visualize the distortion. \n",
        "\n",
        "The camera matrix contains important information about the intrinsic parameters of a camera, such as its focal length, principal point, and skew factor. Specifically, the matrix is a 3x3 matrix with the following form:\n",
        "\n",
        "$$M =\\left\\lbrack\n",
        "\\begin{array}{cccc}\n",
        "f_x & 0 & c_x\\\\\n",
        "0 & f_y & 0 \\\\\n",
        "0 & 0 & 1\\\\\n",
        "\\end{array}\\right\\rbrack$$\n",
        " \n",
        "Here, f_x and f_y represent the focal lengths of the camera in the x and y directions, respectively. The parameters c_x and c_y represent the coordinates of the principal point, which is the point where the optical axis of the camera intersects the image plane. The parameter s represents the skew factor, which is typically zero unless the camera sensor is misaligned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqZrCmzLwM6R"
      },
      "source": [
        "The calibration accuracy of a camera refers to how well the camera parameters have been estimated through calibration. The calibration accuracy is indicated by the total error value of 0.0487 that is printed after calibrating the camera. This value represents the average reprojection error in pixels, which is a measure of the discrepancy between the detected corners in the calibration images and their corresponding positions on the calibration pattern. A good rule of thumb for calibration accuracy is to aim for a reprojection error of less than 1-2 pixels, which indicates that the camera parameters have been estimated with sufficient precision. In this case, the total error value of 0.0487 indicates that the calibration is accurate and meets the desired level of precision. However, it is worth noting that the accuracy of calibration can be affected by various factors such as the quality of the calibration pattern and the calibration images, the number of images used, and the calibration algorithm used. Therefore, it is important to carefully assess the calibration accuracy and consider the factors that may affect it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XppajTAkwM6R"
      },
      "outputs": [],
      "source": [
        "img = cv.imread('cam2_images/11.jpg')\n",
        "undistort(img, mtx, dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVbcZpdwM6S"
      },
      "source": [
        "The image displayed side by side with the original image and the undistorted image shows a comparison between the two images after applying the ***undistort()*** function. On the left side of the image is the original image which shows a distorted view of the chessboard pattern. On the right side of the image is the undistorted image, which shows the corrected view of the chessboard after applying the calibration parameters. The undistorted image is noticeably clearer and more accurate, with straighter lines and less distortion compared to the original image. The undistortion process was achieved using the calibration parameters stored in the cam.yaml file, including the camera matrix and distortion coefficients. The calibration process was deemed accurate with a total error of 0.0487, which is considered a good calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiDv0HNqwM6S"
      },
      "outputs": [],
      "source": [
        "und = cv.imread('undistort.png')\n",
        "undistort_gray = cv.cvtColor(und, cv.COLOR_BGR2GRAY)\n",
        "# Find the chess board corners\n",
        "ret_undistort, corners_undistort = cv.findChessboardCorners(undistort_gray, (CHESSBOARD_CORNER_NUM_X,CHESSBOARD_CORNER_NUM_Y), None)\n",
        "\n",
        "#find corners\n",
        "corners_undistort = cv.cornerSubPix(undistort_gray,corners_undistort, (11,11), (-1,-1), criteria)\n",
        "\n",
        "draw_line(und,corners_undistort)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rj4k2u3fVDxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLrqXkqRVDup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ0OR7RawM6S"
      },
      "source": [
        "\n",
        "\n",
        "The undistorted image with lines drawn on it clearly shows that the image has been successfully undistorted. The lines on the undistorted image are straight and parallel, unlike the lines on the original image, which were distorted due to the camera's lens distortion. This confirms that the camera calibration using the calibration function and the generated camera matrix and distortion coefficients stored in the calibration.yaml file were accurate. The undistorted image can now be used for further image processing or computer vision tasks without being affected by the camera's lens distortion."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2:\n",
        "\n",
        "## 2.3: Find the camera position:\n",
        "\n",
        "![](https://i.imgur.com/GjxG0Nx.png)\n",
        "\n",
        "Suppose we have a 3D space, call the plane containing the road is base plane, and the road view we see on the image is image region. Suppose that center point of the image is  $O$ , the projection point of camera to the base plane is  $P$ . line $OP$  intersect the image region at lower border, call that intersect is point  $L$ . Call  $Pd$  is the projection of  $P$  to the midline. Call  $O′,L′,P′$ . The figure below illustrates this situation.\n",
        "\n",
        "If we ignore the height, the distance from camera to midlines is also the length of line  $PD$ . To solve this problem, we can flat the 3D space to image Now we can work with only image plane\n",
        "\n",
        "![](https://i.imgur.com/vwzxKto.png)\n",
        "\n",
        "### 2.3.1 Calulate  $PP′ $\n",
        "\n",
        "We want to have the value of  $PP'$ , the solution is to find the value of  $OO',LL',OL,LP $\n",
        "\n",
        "1.1/ Find the vlue of  $OL,LP$ \n",
        "\n",
        "We assume that the camera is fixed, so OL an LP must be also fixed as it is correlate with the change of camera. So to get the distance of OL and LP easily, we can simply measure it in the real world\n",
        "\n",
        "### 2.3.2 Find the value of  $OO′,LP$ \n",
        "\n",
        "With the same x coordinate, ratio of pixel and milimeters is unchange for any pairs of  $y1,y2$ , Using this feature, we can determine the distance in milimeters of  $OO′,LL'$,  as each lines is located on same $x$ coordinate.\n",
        "\n",
        "![](https://i.imgur.com/wM8K9sv.png)\n",
        "\n",
        "With the measurement, we will have the ratio (mm/pixel) of pixel and milimeters at lower border axis and center axis. So the value of the these distance: $real distance = ratio * pixel$\n",
        "\n",
        "1.3/ Once we have the values of all the required distances, we can use the Thales theorem to find  $PP′$ .\n",
        "\n",
        "Call A is the intersect point of midline and OL \n",
        "\n",
        "$\\frac{AL}{LO}=\\frac{LL'}{OO'}=>AO=\\frac{OL}{\\frac{LL'}{OO'}-1}\\\\\n",
        "\\frac{AO}{AO+OL+LP}=\\frac{OO'}{PP'}=>PP'=\\frac{OO'*(AO+OL+LP)}{AO}$\n",
        "\n",
        "\n",
        "\n",
        "### 2.3.3 calulcate  $PP_d$ \n",
        "\n",
        "$PPd=PP′sin(L′P′D^)$ . This means that the sine of the angle between the midline and the horizontal axis is equal to the ratio of  $PP_d$  and  $PP′$ . We can use this to find the distance of the camera.\n",
        "\n",
        "### 2.3.4 Accuracy of the algorithm"
      ],
      "metadata": {
        "id": "OrdiKzX7C7Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv()"
      ],
      "metadata": {
        "id": "O6RDTgJDFkdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}